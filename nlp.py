# -*- coding: utf-8 -*-
"""NLP.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10MO4aGWaSTRr9YCmumruO4BlteUuqy0C

*   Merieme AMINE
*   Ibréhime TRAORE

# **Sentiment analysis of New York Times articles' comments**

## **1) Introduction**

Next to classic powers (executive, legislative and judicial), we have a fourth that is the media’s. They publish articles on different themes.. Given the diversity of the articles published and the heterogeneity of the readers, we start from the fact that the opinions given will allow us to understand what the readers think about current news and what one thinks about others’ opinions. Our aim will therefore be threefold: firstly to qualify the reader's sentiment on the subject of the article through his comment; secondly measure the popularity of his comment and finally analyze the feelings of other readers about his comment

## **2) Labelling of comments**

We firstly import comments and replies of April 2018. Then, we only focus on comments
"""

# database 
import pandas as pd
df = pd.read_csv('comment.csv')
#a sample of the data for fast tokenization
print("Shape:  {0}".format(df.shape))
df.head()

"""> Now, we will labelize our comments"""

!pip install pyspark
!pip install transformers

#Import all the packages need to preprocess the data and calculate the sentiment
import nltk
from nltk.stem.wordnet import WordNetLemmatizer
import string
import re
from nltk.corpus import stopwords
from nltk import sent_tokenize, word_tokenize, pos_tag
#from pyspark.sql.functions import udf
#from pyspark.sql.types import *
from nltk.corpus import sentiwordnet as swn
import sys

#Download the needed corpus 
nltk.download('averaged_perceptron_tagger')
nltk.download('wordnet')
nltk.download('sentiwordnet')
nltk.download('punkt')
nltk.download('stopwords')

# Ponctuations
points = stopwords.words('english')
def remove_stopwords(data_str):
    data_str = data_str.lower()
    comment = [word for word in data_str.split(" ") if word not in points]
    return ' '.join(comment)
def clean(astring):
  astring = astring.lower()
  num_re = re.compile('(\\d+)')
  punc_re = re.compile('[%s]' % re.escape(string.punctuation))
  data_str= punc_re.sub(' ', astring)
  astring = num_re.sub(' ', data_str)
  return " ".join(astring.split())

def tokenize(aString):
  a = word_tokenize(aString)
  #only return values long enough
  return a

#extract part of speech
def pos(tokenized_text):
    sent_tag_list = pos_tag(tokenized_text) 
    aList = []
    for word, tag in sent_tag_list:
        tagToUse = ''
        if tag.startswith('J'):
            tagToUse= 'a'
        elif tag.startswith('N'):
            tagToUse= 'n'
        elif tag.startswith('R'):
            tagToUse= 'r'
        elif tag.startswith('V'):
            tagToUse= 'v'
        else:
            continue
        aList.append((word, tagToUse))
    return aList

#lemmatize the commit comments  
lemmatizer = WordNetLemmatizer()
def lemmatize(array_of_word_for_a_comment):
  all_words_in_comment = []
  for word in array_of_word_for_a_comment:
    lemma = lemmatizer.lemmatize(word[0], pos=word[1])
    if not lemma:
      continue
    all_words_in_comment.append([lemma,word[1]])  
  return all_words_in_comment


#Labelize the sentiment 
def labelize(array_of_lemma_tag_for_a_comment):
    alist = [array_of_lemma_tag_for_a_comment]
    totalScore = 0
    count_words_included = 0
    for word in array_of_lemma_tag_for_a_comment:
        synset_forms = list(swn.senti_synsets(word[0], word[1]))
        if not synset_forms:
            continue
        synset = synset_forms[0] 
        totalScore = totalScore + synset.pos_score() - synset.neg_score()
        count_words_included = count_words_included +1
    final_dec = ''
    if count_words_included == 0:
        final_dec =  0  # 'N/A'
    elif totalScore == 0:
        final_dec =1 # 'Neu'        
    elif totalScore/count_words_included < 0:
        final_dec = 2 # 'Neg'
    elif totalScore/count_words_included > 0:
        final_dec = 3 #'Pos'
    return final_dec

def give_token(comment):
  return tokenize(clean(remove_stopwords(comment)))

def give_label(comment):
  return labelize(lemmatize(pos(give_token(comment))))

tokens=df['commentBody'].apply(give_token)
label=df['commentBody'].apply(give_label)

tokens=pd.Series(tokens,name='Tokens')
label=pd.Series(label,name='Labels')
base=pd.concat([label,df.loc[:,'commentBody'],tokens],axis=1,sort=False)
base.head()

def rm(x):
  removal_list=["br","’","s","https","www"]
  return([word for word in x if word not in removal_list])
base.Tokens=base['Tokens'].apply(rm)

base.head()

#topic modelling
from gensim.models import LdaModel
from gensim import corpora
import nltk
from string import punctuation

dictionary = corpora.Dictionary(base.Tokens)
corpus = [dictionary.doc2bow(text) for text in base.Tokens]

ldamodel = LdaModel(corpus, id2word=dictionary, num_topics=4)

!pip install pyLDAvis

import pyLDAvis.gensim
lda_display = pyLDAvis.gensim.prepare(ldamodel, corpus, dictionary)
pyLDAvis.display(lda_display)

"""## **3) Modelling**

# Predicting the popularity of a comment :
"""

import numpy as np
np.quantile(df.recommendations, 0.5)

np.quantile(df.recommendations, 0.8)

df.drop_duplicates().groupby(df.recommendations).size().hist()

"""We will consider that a comment is popular when the number of upvotes exceeds the 80th percentile, in this cas the median number of upvotes is 20."""

X=base.Tokens.array
X = [' '.join(x) for x in X]
print(len(X), X[0])

df['pop'] =  np.where(df['recommendations']>20, 'very popular', (np.where(df['recommendations']>10, 'popular', 'neutral')))

y = df['pop']
print(y)
print(y.shape)

y.value_counts()

from sklearn.model_selection import train_test_split

# create train 60% 
X_train, X_devtest, y_train, y_devtest = train_test_split(X, y, test_size=0.4, random_state=42)
# create dev 20% test 20% (hence 50% from the 40%)
X_dev, X_test, y_dev, y_test = train_test_split(X_devtest, y_devtest, test_size=0.5, random_state=42)

y_dev.value_counts()

y_test.value_counts()

#vectorizing X using CountVectorizer
from sklearn.feature_extraction.text import CountVectorizer
cv = CountVectorizer()
X_train_cv = cv.fit_transform(X_train)
X_train_cv.shape

X_dev_cv = cv.transform(X_dev)
X_dev_cv.shape

from sklearn import preprocessing
lb = preprocessing.LabelBinarizer()
lb_trained = lb.fit(y_train)
y_train_lb = lb_trained.transform(y_train)
y_dev_lb = lb_trained.transform(y_dev)
print(y_train_lb.shape, y_dev_lb.shape)

"""First classification model is a basic Support Vector Machine."""

from sklearn.multiclass import OneVsRestClassifier
from sklearn.svm import LinearSVC
svm = LinearSVC(verbose=1, random_state=42)
clf_svm = OneVsRestClassifier(svm, n_jobs=1 ).fit(X_train_cv.toarray(), y_train_lb)

y_pred_svm = clf_svm.predict(X_dev_cv)
print(y_pred_svm.shape)

print(y_pred_svm[:5])
print(y_dev_lb[:5])

from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc
import matplotlib.pyplot as plt
import seaborn as sn
def show_eval(y_true, y_pred, lb_trained, clf):
  ''' 
  Show eval metrics.  Takes binarized y true and pred along with trained binarizer for label names
  '''
  y_true_names = lb_trained.inverse_transform(y_true)
  y_pred_names = lb_trained.inverse_transform(y_pred)
  print( classification_report(y_true_names, y_pred_names) )
  cm = confusion_matrix(y_true_names, y_pred_names) 
  labels = ['neutral', 'popular', 'very popular']
  df_cm = pd.DataFrame(cm, index=labels, columns=labels)
  # config plot sizes
  sn.set(font_scale=1.2)
  sn.heatmap(df_cm, annot=True, annot_kws={"size": 18}, cmap='coolwarm', linewidth=0.5, fmt="")
  plt.show()

  fpr = dict()
  tpr = dict()
  roc_auc = dict()
  for i, label in enumerate(labels):
    fpr[i], tpr[i], _ = roc_curve(y_true[:, i], y_pred[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])
    print(label, roc_auc[i])

  for i, label in enumerate(labels):
    plt.figure()
    plt.plot(fpr[i], tpr[i], label='ROC curve (area = %0.2f)' % roc_auc[i])
    plt.plot([0, 1], [0, 1], 'k--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver operating characteristic for '+label)
    plt.legend(loc="lower right")
    plt.show()

show_eval(y_dev_lb, y_pred_svm, lb_trained, clf_svm)

"""This model fails to classify the "popular" and "very popular" comments. We will try Random forests classification and compare the outcomes."""

from sklearn.ensemble import RandomForestClassifier
rfc = RandomForestClassifier( max_depth=None, random_state=42,min_samples_split=3, n_jobs=-1, verbose=1)
rfc.fit(X_train_cv, y_train_lb)

y_pred_rfc = rfc.predict(X_dev_cv)
show_eval(y_dev_lb, y_pred_rfc, lb_trained, rfc )

#word2vec as an embedding method
#we need to train it on similar data. Since the field of the comments seem to be political, we will train it on a corpus in the same context.
! wget https://d1p17r2m4rzlbo.cloudfront.net/wp-content/uploads/2016/03/Political-media-DFE.csv

import pandas as pd
df_pol = pd.read_csv('Political-media-DFE.csv', encoding='latin-1')

# Usage of Phrases and Phraser from gensim
import nltk
from gensim.models.phrases import Phrases, Phraser
from nltk.tokenize import TreebankWordTokenizer, TweetTokenizer
nltk.download('punkt')
sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')

def remove_hashtags(tokens):
  tokens = map(lambda x: x.replace('#', ''), tokens)
  return list(tokens)

def remove_url(tokens):
  tokens = filter(lambda x: "http" not in x, tokens)
  return list(tokens)

def remove_html(tokens):
  tokens = filter(lambda x: x[0]+x[-1] != '<>', tokens)
  return list(tokens)

from tqdm import tqdm
def tokenize_url_hashtags(corpus, tweets=False):
  if tweets:
    tokenizer = TweetTokenizer()
  else:
    tokenizer = TreebankWordTokenizer()  
    # Life hack : treebank word tokenizer won't keep html code in one token.
    # To preprocess economics news corpus, use tweettokenizer. 
  tokenized_sentences = []
  for sample in tqdm(corpus):
    # separating sentences
    for sentence in sent_detector.tokenize(sample):
      tokens = tokenizer.tokenize(sentence)
      tokens = remove_url(tokens)
      tokens = remove_html(tokens)
      tokens = remove_hashtags(tokens)
      tokens = list(map(lambda x: x.lower(), tokens))
      tokenized_sentences.append(tokens)
  return tokenized_sentences

cleaned_pol = tokenize_url_hashtags(df_pol.text.array, tweets=True)


from gensim.models.phrases import Phrases, Phraser

def clean_corpus(corpus, threshold=50, tweets=False):
  tokenized_sentences = tokenize_url_hashtags(corpus, tweets=tweets)
  phrases = Phrases(tokenized_sentences, threshold=threshold)

  # This lets you use it with less RAM and faster processing.
  # But it will no longer be possible to update the detector with new training 
  # samples
  phraser = Phraser(phrases)

  # Merging multi-word expressions in the tokenization
  clean_corpus = []
  for sentence in tokenized_sentences:
    clean_corpus.append(phraser[sentence])
  
  return clean_corpus

from gensim.models import Word2Vec
from multiprocessing import cpu_count

cpu = cpu_count()
print('The virtual instance has {} cpus, that will be used to train the word2vec model'.format(cpu))

# We will just get the "WordVectors" parameter from the trained Word2Vec model.
# Otherwise, we could continue training with some more exemples that could be
# fed on the fly to the model.
print("Training the political W2V ...")
pol = Word2Vec(cleaned_pol, size=100, window=5, min_count=3, workers=cpu)
pol.train(cleaned_pol, total_examples=len(cleaned_pol), epochs=10)
pol_wv = pol.wv

# Create a function to transform the corpus into an embedding one
def tokens2vectors(tokenCorpus):
  ''' transforms our X into a list of list of vec (2D array) '''
  new_sample = list()
  for sample in tokenCorpus:
    tweetVecs = list()
    for token in sample.split(' '):
      try: tweetVecs.append(pol_wv.get_vector(token)  )
      except: tweetVecs.append( np.zeros(100) ) 
    new_sample.append(np.mean(tweetVecs, axis=0))
  return np.array(new_sample)


X_train_vec = tokens2vectors(X_train)
print(X_train_vec.shape, X_train_cv.shape)
# print(type(X_train_cv.toarray()), type(X_train_vec))
X_dev_vec = tokens2vectors(X_dev)
X_test_vec = tokens2vectors(X_test)

from sklearn.multiclass import OneVsRestClassifier
from sklearn.svm import LinearSVC
svm = LinearSVC(verbose=1, random_state=42)
clf_svm = OneVsRestClassifier(svm, n_jobs=1).fit(X_train_vec, y_train_lb)
y_pred_svm = clf_svm.predict(X_dev_vec)
show_eval(y_dev_lb, y_pred_svm, lb_trained, svm )

"""No improvement is noticed when changing the embedding method for the SVM model."""

rfc = RandomForestClassifier(n_estimators=300, max_depth=None, random_state=42, n_jobs=-1, verbose=1)
rfc.fit(X_train_vec, y_train_lb)
y_pred_rfc = rfc.predict(X_dev_vec)
show_eval(y_dev_lb, y_pred_rfc, lb_trained, rfc )

"""Same for the randomforests method, there is no improvement when we changed the embedding method. The popularity of a comment is difficult to predict. 

One other method would be working with only two labels to have more observations for the popular modality and thus better outcome.

#Sentiment analysis

Using the labels we created, we try to classify the comments using the previous methods : SVM and Randomforest, using the occurences for the first enmbedding method and comparing it to Word2Vec.
"""

X=base.loc[base['Labels'] != 0].Tokens.array
X = [' '.join(x) for x in X]
print(len(X), X[0])

y = base.loc[base['Labels'] != 0].Labels
print(y)
print(y.shape)

y.value_counts()

# create train 60% 
X_train, X_devtest, y_train, y_devtest = train_test_split(X, y, test_size=0.4, random_state=42)
# create dev 20% test 20% 
X_dev, X_test, y_dev, y_test = train_test_split(X_devtest, y_devtest, test_size=0.5, random_state=42)

#vectorizing X
from sklearn.feature_extraction.text import CountVectorizer
cv = CountVectorizer()
X_train_cv = cv.fit_transform(X_train)
X_train_cv.shape

X_dev_cv = cv.transform(X_dev)
X_dev_cv.shape

from sklearn import preprocessing
lb = preprocessing.LabelBinarizer()
lb_trained = lb.fit(y_train)
y_train_lb = lb_trained.transform(y_train)
y_dev_lb = lb_trained.transform(y_dev)
print(y_train_lb.shape, y_dev_lb.shape)

#SVM
from sklearn.multiclass import OneVsRestClassifier
from sklearn.svm import LinearSVC
svm = LinearSVC(verbose=1, random_state=42)
clf_svm = OneVsRestClassifier(svm, n_jobs=1 ).fit(X_train_cv.toarray(), y_train_lb)

y_pred_svm = clf_svm.predict(X_dev_cv)
print(y_pred_svm.shape)

def show_eval(y_true, y_pred, lb_trained, clf):
  ''' 
  Show eval metrics.  Takes binarized y true and pred along with trained binarizer for label names
  '''
  y_true_names = lb_trained.inverse_transform(y_true)
  y_pred_names = lb_trained.inverse_transform(y_pred)
  print( classification_report(y_true_names, y_pred_names) )
  cm = confusion_matrix(y_true_names, y_pred_names) 
  labels = ['neutral', 'negative', 'positive']
  df_cm = pd.DataFrame(cm, index=labels, columns=labels)
  # config plot sizes
  sn.set(font_scale=1.2)
  sn.heatmap(df_cm, annot=True, annot_kws={"size": 18}, cmap='coolwarm', linewidth=0.5, fmt="")
  plt.show()

  fpr = dict()
  tpr = dict()
  roc_auc = dict()
  for i, label in enumerate(labels):
    fpr[i], tpr[i], _ = roc_curve(y_true[:, i], y_pred[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])
    print(label, roc_auc[i])

  for i, label in enumerate(labels):
    plt.figure()
    plt.plot(fpr[i], tpr[i], label='ROC curve (area = %0.2f)' % roc_auc[i])
    plt.plot([0, 1], [0, 1], 'k--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver operating characteristic for '+label)
    plt.legend(loc="lower right")
    plt.show()

show_eval(y_dev_lb, y_pred_svm, lb_trained, clf_svm)

"""The prediction in this case is fairly good for the negative and postive sentiments but less optimal for the neutra, due to the low number of observations."""

#Randomforest
from sklearn.ensemble import RandomForestClassifier
rfc = RandomForestClassifier(n_estimators=500, max_depth=None, random_state=42,min_samples_split=3, n_jobs=-1, verbose=1)
rfc.fit(X_train_cv, y_train_lb)

y_pred_rfc = rfc.predict(X_dev_cv)
show_eval(y_dev_lb, y_pred_rfc, lb_trained, rfc )

"""The SVM model is better than Randomforests in sentiment analysis in our case, since negative comments seem to be mistaken for positive ones. 

Let us see now with Word2Vec embedding.
"""

#word2vec
X_train_vec = tokens2vectors(X_train)
print(X_train_vec.shape, X_train_cv.shape)
# print(type(X_train_cv.toarray()), type(X_train_vec))
X_dev_vec = tokens2vectors(X_dev)
X_test_vec = tokens2vectors(X_test)

from sklearn.multiclass import OneVsRestClassifier
from sklearn.svm import LinearSVC
svm = LinearSVC(verbose=1, random_state=42)
clf_svm = OneVsRestClassifier(svm, n_jobs=1).fit(X_train_vec, y_train_lb)
y_pred_svm = clf_svm.predict(X_dev_vec)
show_eval(y_dev_lb, y_pred_svm, lb_trained, svm )

"""We see that in this case, the classification is worse that the previous embedding method. The same result is observed for the RandomForest model :"""

rfc = RandomForestClassifier(n_estimators=500, max_depth=None, random_state=42, n_jobs=-1, verbose=1)
rfc.fit(X_train_vec, y_train_lb)
y_pred_rfc = rfc.predict(X_dev_vec)
show_eval(y_dev_lb, y_pred_rfc, lb_trained, rfc )

"""BERT : Pretrained Language Model used for sentiment analysis :"""

import torch
import torch.nn as nn
from torch.utils.data import Dataset
from transformers import BertTokenizer, AutoModel, AutoTokenizer
import pandas as pd
from torch.utils.data import DataLoader
from sklearn import preprocessing
class SSTDataset(Dataset):

    def __init__(self, database, maxlen, model_name='bert-base-uncased'):

        #Store the contents of the file in a pandas dataframe
        self.df = database

        #Initialize the BERT tokenizer
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)

        self.maxlen = maxlen

    def __len__(self):
        return len(self.df)

    def __getitem__(self, index):

        #Selecting the sentence and label at the specified index in the data frame
        sentence = self.df.iloc[index, 1]  #'commentBody'
        #tokens = self.df.iloc[index, 2]
        label = self.df.iloc[index, 0]  #'Labels'
        lb = preprocessing.LabelBinarizer()
        #label = lb.fit(label)

        #Preprocessing the text to be suitable for BERT
        tokens = self.tokenizer.tokenize(sentence) #Tokenize the sentence
        if self.tokenizer.cls_token is None:
          bos_token = self.tokenizer.bos_token
        else:
          bos_token = self.tokenizer.cls_token
          
        if self.tokenizer.sep_token is None:
          eos_token = self.tokenizer.eos_token
        else:
          eos_token = self.tokenizer.sep_token
        
        tokens = [bos_token] + tokens + [eos_token] #Insering the CLS and SEP token in the beginning and end of the sentence
        if len(tokens) < self.maxlen:
            tokens = tokens + [self.tokenizer.pad_token for _ in range(self.maxlen - len(tokens))] #Padding sentences
        else:
            tokens = tokens[:self.maxlen-1] + [eos_token] #Prunning the list to be of specified max length

        tokens_ids = self.tokenizer.convert_tokens_to_ids(tokens) #Obtaining the indices of the tokens in the BERT Vocabulary
        tokens_ids_tensor = torch.tensor(tokens_ids) #Converting the list to a pytorch tensor
        #Obtaining the attention mask i.e a tensor containing 1s for no padded tokens and 0s for padded ones
        attn_mask = (tokens_ids_tensor != 0).long()

        return tokens_ids_tensor, attn_mask, label

#Creating instances of training and validation set
from sklearn.model_selection import train_test_split
training,test = train_test_split(base, test_size=0.2, random_state=42)

train_set = SSTDataset(database = training, maxlen = 60, model_name='bert-base-uncased')
val_set = SSTDataset(database= test, maxlen = 60, model_name='bert-base-uncased')

#Creating instances of training and validation dataloaders
train_loader = DataLoader(train_set, batch_size = 12, num_workers = 5)
val_loader = DataLoader(val_set, batch_size = 12, num_workers = 5)

train_set.__getitem__(2)

class SentimentClassifier(nn.Module):

    def __init__(self, pretrained_model_name='bert-base-uncased'):
        super(SentimentClassifier, self).__init__()
        
        #Loading Mask Language Model 
        self.encoder = AutoModel.from_pretrained(pretrained_model_name)
        #we append an extra layer for Classification (it will be randomly initialized)
        self.cls_layer = nn.Linear(self.encoder.pooler.dense.out_features, 1)

    def forward(self, seq, attn_masks):
        '''
        Inputs:
            -seq : Tensor of shape [B, T] containing token ids of sequences
            -attn_masks : Tensor of shape [B, T] containing attention masks to be used to avoid contibution of PAD tokens
        '''

        #Feeding the input to BERT model to obtain contextualized representations
        # see in the hugging face doc what to input
        #cont_reps = #  self.encoder(..)..
        cont_reps, _ = self.encoder(seq, attention_mask = attn_masks)
        #Obtaining the representation of [CLS] head
        cls_rep = cont_reps[:, 0]

        #Feeding cls_rep to the classifier layer
        logits = self.cls_layer(cls_rep)

        return logits

sentiment_model = SentimentClassifier('bert-base-uncased')
# if gpu mode
sentiment_model = sentiment_model.to("cuda")
# to check if the weights of the model are in gpu : 
# sentiment_model.cls_layer.weight.is_cuda
# can checkout all the layers by running 
sentiment_model

import torch.nn as nn
import torch.optim as optim
# define the loss and optimizer 
criterion = nn.BCEWithLogitsLoss()
opti = optim.Adam(sentiment_model.parameters(), lr = 2e-5)

import pdb
def train(model, criterion, opti, train_loader, val_loader, max_eps=1, gpu=False, print_every=1,validate_every=1):
    if gpu:
      model = model.to("cuda")
    for ep in range(max_eps):
        
        for it, (seq, attn_masks, labels) in enumerate(train_loader):
            #Clear gradients
            opti.zero_grad()  
            #Converting these to cuda tensors
            if gpu:
              seq, attn_masks, labels = seq.cuda(), attn_masks.cuda(), labels.cuda()
            #Obtaining the logits from the model
            logits = model(seq, attn_masks)

            #Computing loss
            loss = criterion(logits.squeeze(-1), labels.float())

            #Backpropagating the gradients
            loss.backward()

            #Optimization step
            opti.step()
            if (it + 1) % print_every == 0:
                accuracy = torch.sum((logits>0).int().squeeze(1)==labels)/float(labels.size(0))
                print("Iteration {} of epoch {} complete. Loss : {}, Accuracy {} ".format(it+1, ep+1, loss.item(),accuracy))
            if it>1000:
              break
        if ep % validate_every==0:
          n_batch_validation = 0
          loss_validation = 0
          accuracy_validation = 0
          for it, (seq, attn_masks, labels) in enumerate(val_loader):
            #Clear gradients
            
            if gpu:
              seq, attn_masks, labels = seq.cuda(), attn_masks.cuda(), labels.cuda()
            #Obtaining the logits from the model
            logits_val = model(seq, attn_masks)
            n_batch_validation+=1
            #Computing loss
           
            _loss = float(criterion(logits_val.squeeze(-1), labels.float()))
            _accu = float(torch.sum((logits_val>0).int().squeeze(1)==labels)/float(labels.size(0)))
           
            loss_validation += _loss
            accuracy_validation += _accu
          print("EVALUATION Validation set : mean loss {} n mean accuracy {}".format(loss_validation/n_batch_validation, accuracy_validation/n_batch_validation))

train(sentiment_model, criterion, opti, train_loader, val_loader, max_eps=10, print_every=100, gpu=True)

"""## **4) Conclusion**"""